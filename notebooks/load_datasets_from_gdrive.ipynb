{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load ML Datasets from Google Drive\n",
    "\n",
    "This notebook downloads and loads the crash prediction datasets from Google Drive for exploratory data analysis (EDA).\n",
    "\n",
    "**Datasets Available:**\n",
    "- Crash-level dataset (train, val, test)\n",
    "- Segment-level dataset (train, val, test)\n",
    "- Raw Texas data (crashes, weather, work zones, traffic)\n",
    "\n",
    "**Setup Required:**\n",
    "1. Install required packages (see cell below)\n",
    "2. Authenticate with Google Drive (first run only)\n",
    "3. Download datasets\n",
    "4. Load and explore!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib\n",
    "# !pip install pandas numpy matplotlib seaborn geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(\"‚úì Imports successful\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Google Drive Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive folder ID (from upload_to_gdrive.py)\n",
    "FOLDER_ID = '1xVGXbxUFHSdSawo2C9wnmABj15wPEX3A'\n",
    "\n",
    "# Local download directory\n",
    "DOWNLOAD_DIR = project_root / 'data' / 'downloaded_from_gdrive'\n",
    "DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Download directory: {DOWNLOAD_DIR}\")\n",
    "print(f\"Google Drive folder: https://drive.google.com/drive/folders/{FOLDER_ID}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Google Drive Authentication & Download Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.auth.transport.requests import Request\n",
    "    from google.oauth2.credentials import Credentials\n",
    "    from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "    from googleapiclient.discovery import build\n",
    "    from googleapiclient.http import MediaIoBaseDownload\n",
    "    from googleapiclient.errors import HttpError\n",
    "    import io\n",
    "    \n",
    "    GDRIVE_AVAILABLE = True\n",
    "    print(\"‚úì Google Drive API packages available\")\n",
    "except ImportError:\n",
    "    GDRIVE_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  Google Drive API packages not installed\")\n",
    "    print(\"   Run: pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive API scopes\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive.readonly']\n",
    "\n",
    "def authenticate_gdrive():\n",
    "    \"\"\"\n",
    "    Authenticate with Google Drive API\n",
    "    \n",
    "    Returns:\n",
    "        service: Google Drive API service object\n",
    "    \"\"\"\n",
    "    creds = None\n",
    "    token_file = project_root / 'token.json'\n",
    "    credentials_file = project_root / 'credentials.json'\n",
    "    \n",
    "    # Load existing token if available\n",
    "    if token_file.exists():\n",
    "        creds = Credentials.from_authorized_user_file(str(token_file), SCOPES)\n",
    "    \n",
    "    # If no valid credentials, authenticate\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            print(\"üîÑ Refreshing expired credentials...\")\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            if not credentials_file.exists():\n",
    "                print(f\"‚ùå Error: {credentials_file} not found\")\n",
    "                print(\"\\nPlease set up Google Drive API credentials:\")\n",
    "                print(\"1. Go to https://console.cloud.google.com/\")\n",
    "                print(\"2. Create/select project\")\n",
    "                print(\"3. Enable Google Drive API\")\n",
    "                print(\"4. Create OAuth 2.0 credentials (Desktop app)\")\n",
    "                print(f\"5. Download as credentials.json to {project_root}\")\n",
    "                return None\n",
    "            \n",
    "            print(\"üîê Authenticating with Google Drive...\")\n",
    "            print(\"   (Browser will open for authorization)\")\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                str(credentials_file), SCOPES\n",
    "            )\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        \n",
    "        # Save credentials for future use\n",
    "        with open(token_file, 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "        print(\"‚úÖ Credentials saved\")\n",
    "    \n",
    "    # Build service\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "    return service\n",
    "\n",
    "def list_files_in_folder(service, folder_id, verbose=True):\n",
    "    \"\"\"\n",
    "    List all files in a Google Drive folder recursively\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts with file info\n",
    "    \"\"\"\n",
    "    try:\n",
    "        files = []\n",
    "        page_token = None\n",
    "        \n",
    "        while True:\n",
    "            query = f\"'{folder_id}' in parents and trashed=false\"\n",
    "            results = service.files().list(\n",
    "                q=query,\n",
    "                spaces='drive',\n",
    "                fields='nextPageToken, files(id, name, mimeType, size)',\n",
    "                pageToken=page_token\n",
    "            ).execute()\n",
    "            \n",
    "            items = results.get('files', [])\n",
    "            files.extend(items)\n",
    "            \n",
    "            page_token = results.get('nextPageToken')\n",
    "            if not page_token:\n",
    "                break\n",
    "        \n",
    "        # Recursively get files from subfolders\n",
    "        all_files = []\n",
    "        for item in files:\n",
    "            if item['mimeType'] == 'application/vnd.google-apps.folder':\n",
    "                # It's a folder - recurse\n",
    "                subfolder_files = list_files_in_folder(service, item['id'], verbose=False)\n",
    "                # Add folder name as prefix\n",
    "                for f in subfolder_files:\n",
    "                    f['path'] = f\"{item['name']}/{f.get('path', f['name'])}\"\n",
    "                all_files.extend(subfolder_files)\n",
    "            else:\n",
    "                # It's a file\n",
    "                item['path'] = item['name']\n",
    "                all_files.append(item)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"‚úì Found {len(all_files)} files\")\n",
    "        \n",
    "        return all_files\n",
    "        \n",
    "    except HttpError as error:\n",
    "        print(f\"‚ùå Error listing files: {error}\")\n",
    "        return []\n",
    "\n",
    "def download_file(service, file_id, file_name, dest_path, verbose=True):\n",
    "    \"\"\"\n",
    "    Download a file from Google Drive\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create parent directories\n",
    "        dest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Download file\n",
    "        request = service.files().get_media(fileId=file_id)\n",
    "        \n",
    "        with open(dest_path, 'wb') as f:\n",
    "            downloader = MediaIoBaseDownload(f, request)\n",
    "            done = False\n",
    "            while not done:\n",
    "                status, done = downloader.next_chunk()\n",
    "                if verbose and status:\n",
    "                    progress = int(status.progress() * 100)\n",
    "                    print(f\"  Downloading {file_name}: {progress}%\", end='\\r')\n",
    "        \n",
    "        if verbose:\n",
    "            file_size = dest_path.stat().st_size / 1024 / 1024\n",
    "            print(f\"  ‚úì Downloaded {file_name} ({file_size:.1f} MB)\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except HttpError as error:\n",
    "        print(f\"  ‚ùå Error downloading {file_name}: {error}\")\n",
    "        return False\n",
    "\n",
    "print(\"‚úì Functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Download Datasets from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate\n",
    "if GDRIVE_AVAILABLE:\n",
    "    service = authenticate_gdrive()\n",
    "    \n",
    "    if service:\n",
    "        print(\"\\n‚úÖ Successfully authenticated with Google Drive\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Authentication failed - please check credentials.json\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Google Drive API not available\")\n",
    "    service = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available files\n",
    "if service:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìÅ Listing files in Google Drive folder...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    files = list_files_in_folder(service, FOLDER_ID)\n",
    "    \n",
    "    # Show file structure\n",
    "    print(\"\\nAvailable files:\")\n",
    "    for f in sorted(files, key=lambda x: x['path']):\n",
    "        size_mb = int(f.get('size', 0)) / 1024 / 1024 if 'size' in f else 0\n",
    "        print(f\"  {f['path']:<60} ({size_mb:>6.1f} MB)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Skipping file listing (authentication required)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download datasets\n",
    "if service:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üì• Downloading datasets...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    downloaded = 0\n",
    "    for file_info in files:\n",
    "        # Skip folders\n",
    "        if file_info['mimeType'] == 'application/vnd.google-apps.folder':\n",
    "            continue\n",
    "        \n",
    "        # Determine local path\n",
    "        local_path = DOWNLOAD_DIR / file_info['path']\n",
    "        \n",
    "        # Skip if already exists\n",
    "        if local_path.exists():\n",
    "            print(f\"  ‚è≠Ô∏è  Skipping {file_info['path']} (already exists)\")\n",
    "            continue\n",
    "        \n",
    "        # Download\n",
    "        success = download_file(service, file_info['id'], file_info['name'], local_path)\n",
    "        if success:\n",
    "            downloaded += 1\n",
    "    \n",
    "    print(f\"\\n‚úÖ Downloaded {downloaded} new files\")\n",
    "    print(f\"   Total files in {DOWNLOAD_DIR}: {len(list(DOWNLOAD_DIR.rglob('*')))}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Skipping download (authentication required)\")\n",
    "    print(\"\\nAlternative: Manually download from:\")\n",
    "    print(f\"  https://drive.google.com/drive/folders/{FOLDER_ID}\")\n",
    "    print(f\"  Save to: {DOWNLOAD_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Datasets into Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset paths\n",
    "CRASH_LEVEL_DIR = DOWNLOAD_DIR / 'crash_level'\n",
    "SEGMENT_LEVEL_DIR = DOWNLOAD_DIR / 'segment_level'\n",
    "\n",
    "# Load crash-level datasets\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä Loading CRASH-LEVEL datasets...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "crash_train = None\n",
    "crash_val = None\n",
    "crash_test = None\n",
    "\n",
    "if (CRASH_LEVEL_DIR / 'train.csv').exists():\n",
    "    crash_train = pd.read_csv(CRASH_LEVEL_DIR / 'train.csv')\n",
    "    print(f\"‚úì Loaded crash_train: {crash_train.shape[0]:,} rows √ó {crash_train.shape[1]} columns\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  train.csv not found\")\n",
    "\n",
    "if (CRASH_LEVEL_DIR / 'val.csv').exists():\n",
    "    crash_val = pd.read_csv(CRASH_LEVEL_DIR / 'val.csv')\n",
    "    print(f\"‚úì Loaded crash_val: {crash_val.shape[0]:,} rows √ó {crash_val.shape[1]} columns\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  val.csv not found\")\n",
    "\n",
    "if (CRASH_LEVEL_DIR / 'test.csv').exists():\n",
    "    crash_test = pd.read_csv(CRASH_LEVEL_DIR / 'test.csv')\n",
    "    print(f\"‚úì Loaded crash_test: {crash_test.shape[0]:,} rows √ó {crash_test.shape[1]} columns\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  test.csv not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load segment-level datasets\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä Loading SEGMENT-LEVEL datasets...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "segment_train = None\n",
    "segment_val = None\n",
    "segment_test = None\n",
    "\n",
    "if (SEGMENT_LEVEL_DIR / 'train.csv').exists():\n",
    "    segment_train = pd.read_csv(SEGMENT_LEVEL_DIR / 'train.csv')\n",
    "    print(f\"‚úì Loaded segment_train: {segment_train.shape[0]:,} rows √ó {segment_train.shape[1]} columns\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  train.csv not found\")\n",
    "\n",
    "if (SEGMENT_LEVEL_DIR / 'val.csv').exists():\n",
    "    segment_val = pd.read_csv(SEGMENT_LEVEL_DIR / 'val.csv')\n",
    "    print(f\"‚úì Loaded segment_val: {segment_val.shape[0]:,} rows √ó {segment_val.shape[1]} columns\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  val.csv not found\")\n",
    "\n",
    "if (SEGMENT_LEVEL_DIR / 'test.csv').exists():\n",
    "    segment_test = pd.read_csv(SEGMENT_LEVEL_DIR / 'test.csv')\n",
    "    print(f\"‚úì Loaded segment_test: {segment_test.shape[0]:,} rows √ó {segment_test.shape[1]} columns\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  test.csv not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Quick Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display crash-level training data overview\n",
    "if crash_train is not None:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìã CRASH-LEVEL TRAINING DATA OVERVIEW\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nShape: {crash_train.shape[0]:,} rows √ó {crash_train.shape[1]} columns\")\n",
    "    \n",
    "    # Target variable distribution\n",
    "    if 'high_severity' in crash_train.columns:\n",
    "        print(\"\\nTarget Variable (high_severity):\")\n",
    "        print(crash_train['high_severity'].value_counts())\n",
    "        print(f\"  High severity rate: {crash_train['high_severity'].mean()*100:.1f}%\")\n",
    "    \n",
    "    # Temporal split\n",
    "    if 'Start_Time' in crash_train.columns:\n",
    "        crash_train['year'] = pd.to_datetime(crash_train['Start_Time']).dt.year\n",
    "        print(\"\\nTemporal Distribution:\")\n",
    "        print(crash_train['year'].value_counts().sort_index())\n",
    "    \n",
    "    # Sample data\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(crash_train.head())\n",
    "    \n",
    "    print(\"\\nData Types:\")\n",
    "    print(crash_train.dtypes.value_counts())\n",
    "    \n",
    "    print(\"\\nMissing Values (top 10):\")\n",
    "    missing = crash_train.isnull().sum().sort_values(ascending=False).head(10)\n",
    "    missing_pct = (missing / len(crash_train) * 100).round(1)\n",
    "    print(pd.DataFrame({'Missing': missing, 'Percent': missing_pct}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize features by source/type\n",
    "if crash_train is not None:\n",
    "    cols = crash_train.columns.tolist()\n",
    "    \n",
    "    feature_categories = {\n",
    "        'Target': [c for c in cols if 'severity' in c.lower()],\n",
    "        'Temporal': [c for c in cols if any(x in c.lower() for x in ['time', 'hour', 'day', 'month', 'year', 'date'])],\n",
    "        'Location': [c for c in cols if any(x in c.lower() for x in ['lat', 'lng', 'lon', 'city', 'county', 'state', 'street', 'zipcode'])],\n",
    "        'Weather': [c for c in cols if any(x in c.lower() for x in ['weather', 'temp', 'wind', 'precip', 'humidity', 'pressure', 'visibility'])],\n",
    "        'Road (OSMnx)': [c for c in cols if c.startswith('osmnx_') or any(x in c.lower() for x in ['highway', 'lanes', 'bridge', 'tunnel', 'oneway'])],\n",
    "        'Road (HPMS)': [c for c in cols if c.startswith('hpms_')],\n",
    "        'Traffic': [c for c in cols if any(x in c.lower() for x in ['aadt', 'traffic'])],\n",
    "        'Work Zones': [c for c in cols if 'wz_' in c or 'work_zone' in c.lower()],\n",
    "        'Lighting': [c for c in cols if 'light' in c.lower()],\n",
    "        'Other': []\n",
    "    }\n",
    "    \n",
    "    # Assign uncategorized columns to 'Other'\n",
    "    categorized = set()\n",
    "    for cat_cols in feature_categories.values():\n",
    "        categorized.update(cat_cols)\n",
    "    feature_categories['Other'] = [c for c in cols if c not in categorized]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìÇ FEATURE CATEGORIES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for category, features in feature_categories.items():\n",
    "        if features:\n",
    "            print(f\"\\n{category} ({len(features)} features):\")\n",
    "            for f in features[:10]:  # Show first 10\n",
    "                print(f\"  - {f}\")\n",
    "            if len(features) > 10:\n",
    "                print(f\"  ... and {len(features) - 10} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Starter EDA Code\n",
    "\n",
    "Below are some starter code snippets for exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "if crash_train is not None and 'high_severity' in crash_train.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Count plot\n",
    "    crash_train['high_severity'].value_counts().plot(kind='bar', ax=axes[0])\n",
    "    axes[0].set_title('Target Variable Distribution (high_severity)', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('High Severity')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_xticklabels(['No (0)', 'Yes (1)'], rotation=0)\n",
    "    \n",
    "    # Add percentages\n",
    "    for i, v in enumerate(crash_train['high_severity'].value_counts().values):\n",
    "        pct = v / len(crash_train) * 100\n",
    "        axes[0].text(i, v + 1000, f'{v:,}\\n({pct:.1f}%)', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Temporal trend\n",
    "    if 'year' in crash_train.columns:\n",
    "        yearly_severity = crash_train.groupby('year')['high_severity'].mean() * 100\n",
    "        yearly_severity.plot(kind='line', marker='o', ax=axes[1])\n",
    "        axes[1].set_title('High Severity Rate by Year', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_xlabel('Year')\n",
    "        axes[1].set_ylabel('High Severity Rate (%)')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather conditions vs severity\n",
    "if crash_train is not None and 'Weather_Condition' in crash_train.columns and 'high_severity' in crash_train.columns:\n",
    "    # Top 10 weather conditions\n",
    "    top_weather = crash_train['Weather_Condition'].value_counts().head(10).index\n",
    "    \n",
    "    weather_severity = crash_train[crash_train['Weather_Condition'].isin(top_weather)].groupby('Weather_Condition')['high_severity'].agg(['mean', 'count'])\n",
    "    weather_severity = weather_severity.sort_values('mean', ascending=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    \n",
    "    # Severity rate by weather\n",
    "    (weather_severity['mean'] * 100).plot(kind='barh', ax=axes[0])\n",
    "    axes[0].set_title('High Severity Rate by Weather Condition', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('High Severity Rate (%)')\n",
    "    axes[0].set_ylabel('Weather Condition')\n",
    "    \n",
    "    # Count by weather\n",
    "    weather_severity['count'].sort_values().plot(kind='barh', ax=axes[1])\n",
    "    axes[1].set_title('Crash Count by Weather Condition', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Number of Crashes')\n",
    "    axes[1].set_ylabel('Weather Condition')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature completeness by source\n",
    "if crash_train is not None:\n",
    "    completeness_data = []\n",
    "    \n",
    "    for category, features in feature_categories.items():\n",
    "        if features and category not in ['Target', 'Other']:\n",
    "            for f in features:\n",
    "                if f in crash_train.columns:\n",
    "                    completeness = crash_train[f].notna().mean() * 100\n",
    "                    completeness_data.append({\n",
    "                        'Category': category,\n",
    "                        'Feature': f,\n",
    "                        'Completeness': completeness\n",
    "                    })\n",
    "    \n",
    "    if completeness_data:\n",
    "        completeness_df = pd.DataFrame(completeness_data)\n",
    "        \n",
    "        # Category averages\n",
    "        category_avg = completeness_df.groupby('Category')['Completeness'].mean().sort_values()\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        category_avg.plot(kind='barh')\n",
    "        plt.title('Average Feature Completeness by Data Source', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Completeness (%)')\n",
    "        plt.ylabel('Data Source')\n",
    "        plt.xlim(0, 100)\n",
    "        plt.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, v in enumerate(category_avg.values):\n",
    "            plt.text(v + 1, i, f'{v:.1f}%', va='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPMS features vs severity\n",
    "if crash_train is not None and 'high_severity' in crash_train.columns:\n",
    "    hpms_features = [c for c in crash_train.columns if c.startswith('hpms_')]\n",
    "    \n",
    "    if hpms_features:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üõ£Ô∏è  HPMS ROAD FEATURES vs SEVERITY\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Speed limit\n",
    "        if 'hpms_speed_limit' in crash_train.columns:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "            \n",
    "            # Distribution\n",
    "            crash_train.boxplot(column='hpms_speed_limit', by='high_severity', ax=axes[0])\n",
    "            axes[0].set_title('Speed Limit by Severity Level', fontsize=12, fontweight='bold')\n",
    "            axes[0].set_xlabel('High Severity')\n",
    "            axes[0].set_ylabel('Speed Limit (mph)')\n",
    "            plt.sca(axes[0])\n",
    "            plt.xticks([1, 2], ['No (0)', 'Yes (1)'])\n",
    "            \n",
    "            # Severity rate by speed bins\n",
    "            speed_bins = [0, 30, 45, 60, 75, 100]\n",
    "            crash_train['speed_bin'] = pd.cut(crash_train['hpms_speed_limit'], bins=speed_bins)\n",
    "            speed_severity = crash_train.groupby('speed_bin')['high_severity'].agg(['mean', 'count'])\n",
    "            \n",
    "            (speed_severity['mean'] * 100).plot(kind='bar', ax=axes[1])\n",
    "            axes[1].set_title('Severity Rate by Speed Limit Range', fontsize=12, fontweight='bold')\n",
    "            axes[1].set_xlabel('Speed Limit Range (mph)')\n",
    "            axes[1].set_ylabel('High Severity Rate (%)')\n",
    "            axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45)\n",
    "            \n",
    "            # Add counts\n",
    "            for i, (idx, row) in enumerate(speed_severity.iterrows()):\n",
    "                axes[1].text(i, row['mean']*100 + 0.5, f\"n={int(row['count']):,}\", \n",
    "                           ha='center', va='bottom', fontsize=9)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            crash_train.drop(columns=['speed_bin'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Dictionary\n",
    "\n",
    "Load and display the data dictionary if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data dictionary\n",
    "data_dict_path = CRASH_LEVEL_DIR / 'DATA_DICTIONARY.md'\n",
    "\n",
    "if data_dict_path.exists():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìñ DATA DICTIONARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    with open(data_dict_path, 'r') as f:\n",
    "        print(f.read())\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  DATA_DICTIONARY.md not found\")\n",
    "    print(f\"   Expected at: {data_dict_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps for EDA\n",
    "\n",
    "**Suggested analyses to explore:**\n",
    "\n",
    "1. **Temporal Patterns**\n",
    "   - Hour of day, day of week, month, season\n",
    "   - Holiday effects\n",
    "   - Temporal trends (2016-2023)\n",
    "\n",
    "2. **Spatial Patterns**\n",
    "   - City/county differences\n",
    "   - Urban vs rural\n",
    "   - Geographic clustering\n",
    "\n",
    "3. **Weather Impact**\n",
    "   - Weather conditions vs severity\n",
    "   - Temperature, precipitation, visibility effects\n",
    "   - Adverse weather combinations\n",
    "\n",
    "4. **Road Characteristics**\n",
    "   - Highway type (HPMS f_system)\n",
    "   - Speed limit ranges\n",
    "   - Lane counts\n",
    "   - Pavement condition (IRI)\n",
    "   - Traffic volume (AADT)\n",
    "\n",
    "5. **Work Zone Effects**\n",
    "   - Crashes in/near work zones\n",
    "   - Work zone density effects\n",
    "\n",
    "6. **Feature Correlations**\n",
    "   - Correlation matrix for numeric features\n",
    "   - Feature importance via random forest\n",
    "\n",
    "7. **Missing Data Analysis**\n",
    "   - Patterns in missingness\n",
    "   - Impact on modeling\n",
    "\n",
    "8. **Class Balance**\n",
    "   - High severity rate across different segments\n",
    "   - Potential need for resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of loaded datasets\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ DATASETS READY FOR ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nAvailable DataFrames:\")\n",
    "print(f\"  - crash_train: {crash_train.shape if crash_train is not None else 'Not loaded'}\")\n",
    "print(f\"  - crash_val: {crash_val.shape if crash_val is not None else 'Not loaded'}\")\n",
    "print(f\"  - crash_test: {crash_test.shape if crash_test is not None else 'Not loaded'}\")\n",
    "print(f\"  - segment_train: {segment_train.shape if segment_train is not None else 'Not loaded'}\")\n",
    "print(f\"  - segment_val: {segment_val.shape if segment_val is not None else 'Not loaded'}\")\n",
    "print(f\"  - segment_test: {segment_test.shape if segment_test is not None else 'Not loaded'}\")\n",
    "print(\"\\nHappy analyzing! üéâ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
