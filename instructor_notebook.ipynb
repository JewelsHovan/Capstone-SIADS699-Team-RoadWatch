{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Texas Crash Prediction - Instructor Setup & Pipeline Notebook\n",
    "\n",
    "**Team Road Watch | SIADS 699 Capstone**\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides an interactive, end-to-end walkthrough for:\n",
    "\n",
    "1. **Environment Setup** - Verify Python environment and dependencies\n",
    "2. **Data Directory Setup** - Create medallion architecture directories\n",
    "3. **Data Verification** - Validate required data files are in place\n",
    "4. **Data Pipeline** - Build ML-ready crash-level datasets\n",
    "5. **Model Training** - Train and evaluate crash severity prediction models\n",
    "6. **Streamlit App** - Launch the interactive dashboard\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- **Python 3.10+**\n",
    "- **~5GB disk space** for data and models\n",
    "- **Data files** downloaded per `DATA_ACQUISITION.md`\n",
    "\n",
    "## Time Estimates\n",
    "\n",
    "| Mode | Pipeline | Training | Total |\n",
    "|------|----------|----------|-------|\n",
    "| Sample (10K) | ~2 min | ~5 min | ~10 min |\n",
    "| Full (466K) | ~30 min | ~15 min | ~1 hour |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Adjust these settings before running the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Edit these settings\n",
    "# =============================================================================\n",
    "\n",
    "# Sample mode: Set to True for quick demos (~10 min), False for full run (~1 hour)\n",
    "SAMPLE_MODE = True\n",
    "\n",
    "# Number of samples to use in sample mode\n",
    "SAMPLE_SIZE = 10_000\n",
    "\n",
    "# Which models to train: 'baseline', 'xgboost', 'catboost', 'lightgbm', 'all'\n",
    "MODELS_TO_TRAIN = 'baseline'  # 'baseline' trains LogisticRegression + RandomForest\n",
    "\n",
    "# Skip segment-level dataset (faster, crash-level is sufficient for demo)\n",
    "SKIP_SEGMENT_LEVEL = True\n",
    "\n",
    "# Skip training if you just want to run the pipeline\n",
    "SKIP_TRAINING = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/julien.hovan/Desktop/MADS/Capstone/Capstone-SIADS699-Team-RoadWatch\n"
     ]
    }
   ],
   "source": [
    "# Cell 1.1: Add project root to Python path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure we're in the project directory\n",
    "notebook_dir = Path.cwd()\n",
    "if (notebook_dir / 'config' / 'paths.py').exists():\n",
    "    PROJECT_ROOT = notebook_dir\n",
    "elif (notebook_dir.parent / 'config' / 'paths.py').exists():\n",
    "    PROJECT_ROOT = notebook_dir.parent\n",
    "else:\n",
    "    raise RuntimeError(\"Cannot find project root. Please run from project directory.\")\n",
    "\n",
    "# Add to Python path\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.14.0\n"
     ]
    }
   ],
   "source": [
    "# Cell 1.2: Check Python version\n",
    "import platform\n",
    "\n",
    "python_version = platform.python_version()\n",
    "major, minor = map(int, python_version.split('.')[:2])\n",
    "\n",
    "if major >= 3 and minor >= 10:\n",
    "    print(f\"Python {python_version}\")\n",
    "else:\n",
    "    print(f\"Python {python_version}\")\n",
    "    print(\"   Python 3.10+ is recommended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package already installed\n"
     ]
    }
   ],
   "source": [
    "# Cell 1.3: Install package in editable mode (if not already installed)\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# Check if package is installed\n",
    "try:\n",
    "    from config.paths import PROJECT_ROOT as _\n",
    "    print(\"Package already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing package...\")\n",
    "    # Use uv if available, otherwise pip\n",
    "    if shutil.which(\"uv\"):\n",
    "        !uv pip install -e .\n",
    "    else:\n",
    "        !pip install -e .\n",
    "    print(\"Package installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency Check:\n",
      "----------------------------------------\n",
      "  pandas: MISSING: No module named 'pandas'\n",
      "  numpy: MISSING: No module named 'numpy'\n",
      "  scikit-learn: MISSING: No module named 'sklearn'\n",
      "  geopandas: MISSING: No module named 'geopandas'\n",
      "  xgboost: MISSING: No module named 'xgboost'\n",
      "  mlflow: MISSING: No module named 'mlflow'\n",
      "  streamlit: MISSING: No module named 'streamlit'\n",
      "\n",
      "Some dependencies are missing. Run: pip install -e .\n"
     ]
    }
   ],
   "source": [
    "# Cell 1.4: Verify core imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "checks = {}\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    checks['pandas'] = f\"v{pd.__version__}\"\n",
    "except ImportError as e:\n",
    "    checks['pandas'] = f\"MISSING: {e}\"\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    checks['numpy'] = f\"v{np.__version__}\"\n",
    "except ImportError as e:\n",
    "    checks['numpy'] = f\"MISSING: {e}\"\n",
    "\n",
    "try:\n",
    "    import sklearn\n",
    "    checks['scikit-learn'] = f\"v{sklearn.__version__}\"\n",
    "except ImportError as e:\n",
    "    checks['scikit-learn'] = f\"MISSING: {e}\"\n",
    "\n",
    "try:\n",
    "    import geopandas as gpd\n",
    "    checks['geopandas'] = f\"v{gpd.__version__}\"\n",
    "except ImportError as e:\n",
    "    checks['geopandas'] = f\"MISSING: {e}\"\n",
    "\n",
    "try:\n",
    "    import xgboost\n",
    "    checks['xgboost'] = f\"v{xgboost.__version__}\"\n",
    "except ImportError as e:\n",
    "    checks['xgboost'] = f\"MISSING: {e}\"\n",
    "\n",
    "try:\n",
    "    import mlflow\n",
    "    checks['mlflow'] = f\"v{mlflow.__version__}\"\n",
    "except ImportError as e:\n",
    "    checks['mlflow'] = f\"MISSING: {e}\"\n",
    "\n",
    "try:\n",
    "    import streamlit\n",
    "    checks['streamlit'] = f\"v{streamlit.__version__}\"\n",
    "except ImportError as e:\n",
    "    checks['streamlit'] = f\"MISSING: {e}\"\n",
    "\n",
    "print(\"Dependency Check:\")\n",
    "print(\"-\" * 40)\n",
    "all_ok = True\n",
    "for pkg, status in checks.items():\n",
    "    if \"MISSING\" in status:\n",
    "        print(f\"  {pkg}: {status}\")\n",
    "        all_ok = False\n",
    "    else:\n",
    "        print(f\"  {pkg}: {status}\")\n",
    "\n",
    "if all_ok:\n",
    "    print(\"\\nAll dependencies available\")\n",
    "else:\n",
    "    print(\"\\nSome dependencies are missing. Run: pip install -e .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1.5: macOS XGBoost check (libomp dependency)\n",
    "import platform\n",
    "\n",
    "if platform.system() == 'Darwin':\n",
    "    print(\"macOS detected - checking XGBoost libomp dependency...\")\n",
    "    try:\n",
    "        import xgboost\n",
    "        # Try a simple operation to verify libomp\n",
    "        import numpy as np\n",
    "        xgb_test = xgboost.DMatrix(np.array([[1, 2], [3, 4]]))\n",
    "        print(\"  XGBoost works correctly\")\n",
    "    except Exception as e:\n",
    "        print(f\"  XGBoost issue: {e}\")\n",
    "        print(\"  If XGBoost fails, try: brew install libomp\")\n",
    "else:\n",
    "    print(f\"Platform: {platform.system()} - XGBoost should work without additional setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Data Directory Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2.1: Create medallion architecture directories\n",
    "from config.paths import (\n",
    "    ensure_directories,\n",
    "    DATA_ROOT,\n",
    "    BRONZE, BRONZE_TEXAS,\n",
    "    SILVER, SILVER_TEXAS,\n",
    "    GOLD, GOLD_ML_DATASETS,\n",
    "    CRASH_LEVEL_ML, SEGMENT_LEVEL_ML,\n",
    "    TEXAS_BRONZE_CRASHES, TEXAS_SILVER_ROADWAY,\n",
    "    DEFAULT_CRASH_FILE, DEFAULT_HPMS_FILE\n",
    ")\n",
    "\n",
    "print(\"Creating directory structure...\\n\")\n",
    "ensure_directories()\n",
    "print(\"Directory structure ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2.2: Display directory tree\n",
    "def print_tree(directory, prefix='', max_depth=3, current_depth=0):\n",
    "    \"\"\"Print directory tree with depth limit\"\"\"\n",
    "    if current_depth >= max_depth:\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        entries = sorted([e for e in directory.iterdir() if e.is_dir()])\n",
    "    except PermissionError:\n",
    "        return\n",
    "    \n",
    "    for i, entry in enumerate(entries):\n",
    "        is_last = (i == len(entries) - 1)\n",
    "        connector = '' if is_last else ''\n",
    "        print(f\"{prefix}{connector}{entry.name}/\")\n",
    "        \n",
    "        extension = '    ' if is_last else '   '\n",
    "        print_tree(entry, prefix + extension, max_depth, current_depth + 1)\n",
    "\n",
    "print(\"Data Directory Structure:\")\n",
    "print(f\"{DATA_ROOT.name}/\")\n",
    "print_tree(DATA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2.3: Show expected data file locations\n",
    "print(\"Expected Data File Locations:\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Bronze Layer (raw data):\")\n",
    "print(f\"  Crash data: {DEFAULT_CRASH_FILE}\")\n",
    "print()\n",
    "print(\"Silver Layer (cleaned data):\")\n",
    "print(f\"  HPMS data:  {DEFAULT_HPMS_FILE}\")\n",
    "print()\n",
    "print(\"See DATA_ACQUISITION.md for download instructions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Data Verification\n",
    "\n",
    "Verify that the required data files are in place before running the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.1: Check if data files exist\n",
    "import pandas as pd\n",
    "\n",
    "def check_file(file_path, description):\n",
    "    \"\"\"Check if file exists and return status\"\"\"\n",
    "    if file_path.exists():\n",
    "        size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "        return True, f\"{size_mb:.1f} MB\"\n",
    "    return False, \"NOT FOUND\"\n",
    "\n",
    "print(\"Data File Verification:\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Check crash data\n",
    "crash_exists, crash_info = check_file(DEFAULT_CRASH_FILE, \"Crash Data\")\n",
    "status = \"\" if crash_exists else \"\"\n",
    "print(f\"{status} Crash Data (Kaggle US Accidents): {crash_info}\")\n",
    "print(f\"    Path: {DEFAULT_CRASH_FILE}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Check HPMS data\n",
    "hpms_exists, hpms_info = check_file(DEFAULT_HPMS_FILE, \"HPMS Data\")\n",
    "status = \"\" if hpms_exists else \"\"\n",
    "print(f\"{status} HPMS Data (Texas 2023): {hpms_info}\")\n",
    "print(f\"    Path: {DEFAULT_HPMS_FILE}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Summary\n",
    "if crash_exists and hpms_exists:\n",
    "    print(\"All required data files found!\")\n",
    "    DATA_READY = True\n",
    "else:\n",
    "    print(\"Missing data files. See DATA_ACQUISITION.md for download instructions.\")\n",
    "    DATA_READY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.2: Validate crash data structure\n",
    "if DEFAULT_CRASH_FILE.exists():\n",
    "    print(\"Validating crash data...\")\n",
    "    \n",
    "    # Read sample to check structure\n",
    "    df_sample = pd.read_csv(DEFAULT_CRASH_FILE, nrows=100)\n",
    "    \n",
    "    required_cols = ['ID', 'Start_Time', 'Start_Lat', 'Start_Lng', 'Severity']\n",
    "    missing_cols = [col for col in required_cols if col not in df_sample.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"  Missing required columns: {missing_cols}\")\n",
    "    else:\n",
    "        print(f\"  Required columns present\")\n",
    "    \n",
    "    # Count total rows\n",
    "    print(\"  Counting total rows (this may take a moment)...\")\n",
    "    total_rows = sum(1 for _ in open(DEFAULT_CRASH_FILE)) - 1\n",
    "    print(f\"  Total crashes: {total_rows:,}\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\nSample data (first 3 rows):\")\n",
    "    display(df_sample[required_cols].head(3))\n",
    "else:\n",
    "    print(\"Crash data file not found. Skipping validation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3.3: Validate HPMS data structure\n",
    "if DEFAULT_HPMS_FILE.exists():\n",
    "    import geopandas as gpd\n",
    "    \n",
    "    print(\"Validating HPMS data...\")\n",
    "    \n",
    "    # Read sample\n",
    "    gdf_sample = gpd.read_file(DEFAULT_HPMS_FILE, rows=100)\n",
    "    \n",
    "    print(f\"  Columns: {len(gdf_sample.columns)}\")\n",
    "    print(f\"  CRS: {gdf_sample.crs}\")\n",
    "    print(f\"  Geometry type: {gdf_sample.geometry.geom_type.mode()[0]}\")\n",
    "    \n",
    "    # Count total features\n",
    "    print(\"  Counting total road segments (this may take a moment)...\")\n",
    "    gdf_full = gpd.read_file(DEFAULT_HPMS_FILE)\n",
    "    print(f\"  Total road segments: {len(gdf_full):,}\")\n",
    "    \n",
    "    # Key columns\n",
    "    key_cols = ['speed_limit', 'through_lanes', 'aadt', 'f_system']\n",
    "    available_key_cols = [col for col in key_cols if col in gdf_sample.columns]\n",
    "    print(f\"  Key columns available: {available_key_cols}\")\n",
    "else:\n",
    "    print(\"HPMS data file not found. Skipping validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Run Data Pipeline\n",
    "\n",
    "Build ML-ready datasets from raw data. This section:\n",
    "1. Loads crash data from Bronze layer\n",
    "2. Integrates HPMS road characteristics\n",
    "3. Engineers features\n",
    "4. Creates train/val/test splits\n",
    "5. Saves to Gold layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Pre-flight Check:\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SAMPLE_MODE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33mSAMPLE\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[43mSAMPLE_MODE\u001b[49m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mFULL\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m SAMPLE_MODE:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSample size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSAMPLE_SIZE\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m records\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'SAMPLE_MODE' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 4.1: Pre-flight check before pipeline\n",
    "print(\"Pipeline Pre-flight Check:\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(f\"Mode: {'SAMPLE' if SAMPLE_MODE else 'FULL'}\")\n",
    "if SAMPLE_MODE:\n",
    "    print(f\"Sample size: {SAMPLE_SIZE:,} records\")\n",
    "print(f\"Skip segment-level: {SKIP_SEGMENT_LEVEL}\")\n",
    "print()\n",
    "\n",
    "# Check if data is ready\n",
    "if not (DEFAULT_CRASH_FILE.exists() and DEFAULT_HPMS_FILE.exists()):\n",
    "    print(\"Data files not found! Cannot run pipeline.\")\n",
    "    print(\"Please download data per DATA_ACQUISITION.md\")\n",
    "    PIPELINE_READY = False\n",
    "else:\n",
    "    print(\"Data files found. Pipeline ready to run.\")\n",
    "    PIPELINE_READY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4.2: Run crash-level pipeline\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if PIPELINE_READY:\n",
    "    print(\"Building Crash-Level Dataset\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    # Build command\n",
    "    cmd = [sys.executable, 'data_engineering/datasets/build_crash_level_dataset.py']\n",
    "    \n",
    "    if SAMPLE_MODE:\n",
    "        cmd.extend(['--sample', str(SAMPLE_SIZE)])\n",
    "        print(f\"Running in SAMPLE mode with {SAMPLE_SIZE:,} records...\")\n",
    "    else:\n",
    "        print(\"Running in FULL mode (this will take ~30 minutes)...\")\n",
    "    \n",
    "    print(f\"\\nCommand: {' '.join(cmd)}\\n\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Run with live output\n",
    "    process = subprocess.Popen(\n",
    "        cmd,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        universal_newlines=True,\n",
    "        cwd=str(PROJECT_ROOT),\n",
    "        env={**os.environ, 'PYTHONPATH': str(PROJECT_ROOT)}\n",
    "    )\n",
    "    \n",
    "    # Stream output\n",
    "    for line in process.stdout:\n",
    "        print(line, end='')\n",
    "    \n",
    "    process.wait()\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    if process.returncode == 0:\n",
    "        print(\"\\nCrash-level dataset built successfully!\")\n",
    "        CRASH_DATASET_BUILT = True\n",
    "    else:\n",
    "        print(f\"\\nPipeline failed with exit code {process.returncode}\")\n",
    "        CRASH_DATASET_BUILT = False\n",
    "else:\n",
    "    print(\"Pipeline not ready. Please check data files.\")\n",
    "    CRASH_DATASET_BUILT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4.3: Run segment-level pipeline (optional)\n",
    "if PIPELINE_READY and not SKIP_SEGMENT_LEVEL:\n",
    "    print(\"Building Segment-Level Dataset\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    cmd = [sys.executable, 'data_engineering/datasets/build_segment_level_dataset.py']\n",
    "    \n",
    "    if SAMPLE_MODE:\n",
    "        cmd.extend(['--sample', str(SAMPLE_SIZE)])\n",
    "    \n",
    "    print(f\"Command: {' '.join(cmd)}\\n\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    process = subprocess.Popen(\n",
    "        cmd,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        universal_newlines=True,\n",
    "        cwd=str(PROJECT_ROOT),\n",
    "        env={**os.environ, 'PYTHONPATH': str(PROJECT_ROOT)}\n",
    "    )\n",
    "    \n",
    "    for line in process.stdout:\n",
    "        print(line, end='')\n",
    "    \n",
    "    process.wait()\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    if process.returncode == 0:\n",
    "        print(\"\\nSegment-level dataset built successfully!\")\n",
    "    else:\n",
    "        print(f\"\\nPipeline failed with exit code {process.returncode}\")\n",
    "else:\n",
    "    print(\"Skipping segment-level dataset (SKIP_SEGMENT_LEVEL=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Explore Generated Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5.1: Check generated datasets\n",
    "print(\"Generated Datasets:\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Check crash-level datasets\n",
    "crash_files = {\n",
    "    'train': CRASH_LEVEL_ML / 'train_latest.csv',\n",
    "    'val': CRASH_LEVEL_ML / 'val_latest.csv',\n",
    "    'test': CRASH_LEVEL_ML / 'test_latest.csv'\n",
    "}\n",
    "\n",
    "print(\"Crash-Level Datasets:\")\n",
    "for split, path in crash_files.items():\n",
    "    if path.exists():\n",
    "        size_mb = path.stat().st_size / (1024 * 1024)\n",
    "        df = pd.read_csv(path, nrows=1)\n",
    "        total_rows = sum(1 for _ in open(path)) - 1\n",
    "        print(f\"  {split}: {total_rows:,} rows, {len(df.columns)} columns, {size_mb:.1f} MB\")\n",
    "    else:\n",
    "        print(f\"  {split}: NOT FOUND\")\n",
    "\n",
    "# Check segment-level datasets\n",
    "if not SKIP_SEGMENT_LEVEL:\n",
    "    print(\"\\nSegment-Level Datasets:\")\n",
    "    segment_files = {\n",
    "        'train': SEGMENT_LEVEL_ML / 'train_latest.csv',\n",
    "        'val': SEGMENT_LEVEL_ML / 'val_latest.csv', \n",
    "        'test': SEGMENT_LEVEL_ML / 'test_latest.csv'\n",
    "    }\n",
    "    for split, path in segment_files.items():\n",
    "        if path.exists():\n",
    "            size_mb = path.stat().st_size / (1024 * 1024)\n",
    "            print(f\"  {split}: {size_mb:.1f} MB\")\n",
    "        else:\n",
    "            print(f\"  {split}: NOT FOUND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5.2: Load and explore crash-level training data\n",
    "train_path = CRASH_LEVEL_ML / 'train_latest.csv'\n",
    "\n",
    "if train_path.exists():\n",
    "    print(\"Loading training dataset...\")\n",
    "    train_df = pd.read_csv(train_path, low_memory=False)\n",
    "    \n",
    "    print(f\"\\nDataset shape: {train_df.shape}\")\n",
    "    print(f\"Memory usage: {train_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    print(\"\\nColumn Types:\")\n",
    "    print(train_df.dtypes.value_counts())\n",
    "    \n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(train_df.head())\n",
    "else:\n",
    "    print(\"Training data not found. Run the pipeline first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5.3: Target variable distribution\n",
    "if train_path.exists() and 'high_severity' in train_df.columns:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    print(\"Target Variable Distribution:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    target_counts = train_df['high_severity'].value_counts()\n",
    "    target_pct = train_df['high_severity'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    print(f\"\\nClass 0 (Low Severity):  {target_counts[0]:,} ({target_pct[0]:.1f}%)\")\n",
    "    print(f\"Class 1 (High Severity): {target_counts[1]:,} ({target_pct[1]:.1f}%)\")\n",
    "    \n",
    "    # Simple bar chart\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    bars = ax.bar(['Low Severity (0)', 'High Severity (1)'], \n",
    "                  [target_counts[0], target_counts[1]],\n",
    "                  color=['#2ecc71', '#e74c3c'])\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Target Variable Distribution')\n",
    "    \n",
    "    # Add count labels\n",
    "    for bar, count in zip(bars, [target_counts[0], target_counts[1]]):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1000,\n",
    "                f'{count:,}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Target variable not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Model Training\n",
    "\n",
    "Train crash severity prediction models with MLflow tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6.1: Check if training should run\n",
    "if SKIP_TRAINING:\n",
    "    print(\"Training skipped (SKIP_TRAINING=True)\")\n",
    "    TRAINING_READY = False\n",
    "elif not (CRASH_LEVEL_ML / 'train_latest.csv').exists():\n",
    "    print(\"Training data not found. Run the pipeline first.\")\n",
    "    TRAINING_READY = False\n",
    "else:\n",
    "    print(\"Training Configuration:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Models to train: {MODELS_TO_TRAIN}\")\n",
    "    print(f\"Dataset: crash-level\")\n",
    "    print()\n",
    "    print(\"Ready to train!\")\n",
    "    TRAINING_READY = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6.2: Train models\n",
    "if TRAINING_READY:\n",
    "    print(\"Training Models\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    cmd = [\n",
    "        sys.executable, '-m', 'ml_engineering.train_with_mlflow',\n",
    "        '--dataset', 'crash',\n",
    "        '--model', MODELS_TO_TRAIN\n",
    "    ]\n",
    "    \n",
    "    print(f\"Command: {' '.join(cmd)}\\n\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    process = subprocess.Popen(\n",
    "        cmd,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        universal_newlines=True,\n",
    "        cwd=str(PROJECT_ROOT),\n",
    "        env={**os.environ, 'PYTHONPATH': str(PROJECT_ROOT)}\n",
    "    )\n",
    "    \n",
    "    for line in process.stdout:\n",
    "        print(line, end='')\n",
    "    \n",
    "    process.wait()\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    if process.returncode == 0:\n",
    "        print(\"\\nTraining completed successfully!\")\n",
    "        TRAINING_COMPLETE = True\n",
    "    else:\n",
    "        print(f\"\\nTraining failed with exit code {process.returncode}\")\n",
    "        TRAINING_COMPLETE = False\n",
    "else:\n",
    "    print(\"Training not ready or skipped.\")\n",
    "    TRAINING_COMPLETE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6.3: List trained models\n",
    "from pathlib import Path\n",
    "\n",
    "models_dir = PROJECT_ROOT / 'models' / 'artifacts'\n",
    "\n",
    "if models_dir.exists():\n",
    "    print(\"Trained Model Artifacts:\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    model_dirs = sorted([d for d in models_dir.iterdir() if d.is_dir()])\n",
    "    \n",
    "    for model_dir in model_dirs:\n",
    "        model_file = model_dir / 'model.pkl'\n",
    "        metrics_file = model_dir / 'metrics.json'\n",
    "        \n",
    "        if model_file.exists():\n",
    "            size_mb = model_file.stat().st_size / (1024 * 1024)\n",
    "            \n",
    "            # Try to load metrics\n",
    "            metrics_str = \"\"\n",
    "            if metrics_file.exists():\n",
    "                import json\n",
    "                with open(metrics_file) as f:\n",
    "                    metrics = json.load(f)\n",
    "                    if 'roc_auc' in metrics:\n",
    "                        metrics_str = f\"AUC={metrics['roc_auc']:.3f}\"\n",
    "            \n",
    "            print(f\"  {model_dir.name}\")\n",
    "            print(f\"      Size: {size_mb:.1f} MB  {metrics_str}\")\n",
    "else:\n",
    "    print(\"No trained models found. Run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Launch Streamlit App\n",
    "\n",
    "Start the interactive dashboard for exploring crash data and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7.1: Check Streamlit app\n",
    "app_path = PROJECT_ROOT / 'app' / 'app.py'\n",
    "\n",
    "if app_path.exists():\n",
    "    print(\"Streamlit App Ready\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"\\nApp location: {app_path}\")\n",
    "    print(\"\\nTo launch manually, run:\")\n",
    "    print(f\"  streamlit run {app_path}\")\n",
    "    STREAMLIT_READY = True\n",
    "else:\n",
    "    print(f\"Streamlit app not found at {app_path}\")\n",
    "    STREAMLIT_READY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7.2: Launch Streamlit (runs in background)\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "if STREAMLIT_READY:\n",
    "    print(\"Launching Streamlit App...\")\n",
    "    print()\n",
    "    \n",
    "    # Start Streamlit in background\n",
    "    streamlit_proc = subprocess.Popen(\n",
    "        ['streamlit', 'run', str(app_path), '--server.headless=true'],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        cwd=str(PROJECT_ROOT)\n",
    "    )\n",
    "    \n",
    "    # Wait for server to start\n",
    "    print(\"Waiting for server to start...\")\n",
    "    time.sleep(3)\n",
    "    \n",
    "    if streamlit_proc.poll() is None:  # Process still running\n",
    "        print(\"\\nStreamlit server started!\")\n",
    "        print()\n",
    "        print(\"   Open in browser: http://localhost:8501\")\n",
    "        print()\n",
    "        print(\"Run the next cell to stop the server when done.\")\n",
    "        \n",
    "        # Store process reference\n",
    "        _STREAMLIT_PROC = streamlit_proc\n",
    "    else:\n",
    "        print(\"Failed to start Streamlit. Check for errors.\")\n",
    "        stdout, stderr = streamlit_proc.communicate()\n",
    "        print(stderr.decode())\n",
    "else:\n",
    "    print(\"Streamlit not ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7.3: Stop Streamlit server\n",
    "if '_STREAMLIT_PROC' in dir() and _STREAMLIT_PROC is not None:\n",
    "    _STREAMLIT_PROC.terminate()\n",
    "    _STREAMLIT_PROC.wait()\n",
    "    print(\"Streamlit server stopped.\")\n",
    "    _STREAMLIT_PROC = None\n",
    "else:\n",
    "    print(\"No Streamlit server running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8: Validation Checklist\n",
    "\n",
    "Summary of completed steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8.1: Final validation checklist\n",
    "print(\"Validation Checklist\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Check each item\n",
    "checks = [\n",
    "    (\"Environment installed\", True),  # If we got this far, environment is set up\n",
    "    (\"Data directories created\", DATA_ROOT.exists()),\n",
    "    (\"Crash data available\", DEFAULT_CRASH_FILE.exists()),\n",
    "    (\"HPMS data available\", DEFAULT_HPMS_FILE.exists()),\n",
    "    (\"Pipeline completed\", (CRASH_LEVEL_ML / 'train_latest.csv').exists()),\n",
    "    (\"Models trained\", (PROJECT_ROOT / 'models' / 'artifacts').exists() and \n",
    "                       any((PROJECT_ROOT / 'models' / 'artifacts').iterdir())),\n",
    "    (\"Streamlit app available\", (PROJECT_ROOT / 'app' / 'app.py').exists()),\n",
    "]\n",
    "\n",
    "for item, status in checks:\n",
    "    icon = \"\" if status else \"\"\n",
    "    print(f\"  {icon} {item}\")\n",
    "\n",
    "print()\n",
    "completed = sum(1 for _, s in checks if s)\n",
    "total = len(checks)\n",
    "print(f\"Progress: {completed}/{total} steps complete\")\n",
    "\n",
    "if completed == total:\n",
    "    print(\"\\nAll steps completed! The project is fully set up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix: Useful Commands\n",
    "\n",
    "Quick reference for common operations:\n",
    "\n",
    "```bash\n",
    "# Run full pipeline\n",
    "python scripts/run_pipeline.py\n",
    "\n",
    "# Run sample pipeline (faster)\n",
    "python scripts/run_pipeline.py --sample 10000\n",
    "\n",
    "# Train all models\n",
    "python -m ml_engineering.train_with_mlflow --dataset crash --model all\n",
    "\n",
    "# Train specific model\n",
    "python -m ml_engineering.train_with_mlflow --dataset crash --model xgboost\n",
    "\n",
    "# Launch Streamlit\n",
    "streamlit run app/app.py\n",
    "\n",
    "# View MLflow runs\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "*Team Road Watch | SIADS 699 Capstone*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
